{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# numpy - helps to work with arrays, matplotlib - visualization, pandas - data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# X: features - independent variable - these are the data used to predict our result\n",
    "# y: result - dependent variable - generally last column - the value which is to be predicted by our model\n",
    "# iloc - locating the values [rows, columns]; colon indicates all rows of the selected columns\n",
    "# [:-1] means except last column as python excludes the upper bound in range & -1 is the index of the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "\n",
    "# results shows a matrix of all features/independent variables x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "\n",
    "# results shows dependent variable vector y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset is too large, instead of checking the dataset, just apply the below code bcoz even if there is no missing values, then it'll leave the dataset the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "# imputing is basically a strategy to handle missing data\n",
    "# SimpleImputer is a class, so need an instance(object) ['imputer' in our case] of this to make use of it\n",
    "#fit is a method of SimpleImputer class & it calculates the required values[in our case 'mean'],\n",
    "# however it requires all numerical col as arguments, hence [1:3]\n",
    "# transform func will replace the missing values in those columns with the calculated mean & return those completed columns\n",
    "# then we just update those numerical columns in our original 'X' dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "#approach is splitting the categorical column into each individual columns, by having their own binary\n",
    "# values, eg. france - 1 0 0, spain - 0 1 0, germany - 0 0 1\n",
    "#basically assigning numerical values to our categorical data, however not such simple order like 1,2,3 \n",
    "# that the model thinks these values has a pattern or something\n",
    "#ct - object of ColumnTransformer class, arguments -> transformers - '0' specifies the index of col to be encoded\n",
    "# remainder - 'passthrough' specifies that we need to keep the other col which are not categorical\n",
    "#ct has a method c/a fit_transform which does both tasks of fit & transform in one, however X is \n",
    "# a numpy array but the fit_transform doesn't return a numpy array, which will cause an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# basically converting those yes & no in the last column to 1 & 0, so machine can easily understand it\n",
    "# we use LableEncoder() class for this\n",
    "# no need to convert this to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why splitting before feature scaling?\n",
    "# feature scaling will scale the dataset using mean & st deviation such that 1 feature doesn't dominate.\n",
    "#Feature scaling should be done after dataset splitting because the model will be fitted only with the training\n",
    "# set and thus the parameters of scaling should be generated only from the training set too \n",
    "# then using these generated parameters from the training set, testing set can be scaled\n",
    "# new scaling for testing set should not be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# so train_test_split class returns 4 variables shown above after providing these arguments\n",
    "# random_state = 42 is just the industry standard that signifies to select the random values for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 35.0 58000.0]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to apply to every model, will only be use in certain model techniques\n",
    "# bcoz many models have constant coeff, they compensate if the value is an outlier\n",
    "# basically uses two techniques: standardisation(values b/w -3 & 3) & normalisation(values b/w 0 & 1) to scale the values\n",
    "# always go for standardisation bcoz it works well in all cases while normalisa. only works on normal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling should not be applied to the dummy variables created through encoding categorical\n",
    "# data bcoz then that values (binary in our case before) will lose their meaning & also they are in\n",
    "# 0s & 1s which is in the range of -3 & +3 so no need.\n",
    "# so, only apply feature scaling for the numerical features which contain non-dummy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:,3:] = sc.fit_transform(X_train[:,3:])\n",
    "X_test[:,3:] = sc.transform(X_test[:,3:])\n",
    "\n",
    "# why columns selected from 3 onwards? bcoz, the dummy variables columns of X_train has the index 0,1,2.\n",
    "#why only transform for X_test? bcoz if it's fit again then new scaler will be generated, but these\n",
    "# data should only be scaled using the fit from the training set, so they are congruent with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we only applied feature scaling to X (features) but not to y (dependant var),\n",
    "# bcoz in this dataset y is 0 or 1 only, hence no need as already in the scale range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 -0.7529426005471072 -0.6260377781240918]\n",
      " [1.0 0.0 0.0 1.008453807952985 1.0130429500553495]\n",
      " [1.0 0.0 0.0 1.7912966561752484 1.8325833141450703]\n",
      " [0.0 1.0 0.0 -1.7314961608249362 -1.0943465576039322]\n",
      " [1.0 0.0 0.0 -0.3615211764359756 0.42765697570554906]\n",
      " [0.0 1.0 0.0 0.22561095973072184 0.05040823668012247]\n",
      " [0.0 0.0 1.0 -0.16581046438040975 -0.27480619351421154]\n",
      " [0.0 0.0 1.0 -0.013591021670525094 -1.3285009473438525]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 2.1827180802863797 2.3008920936249107]\n",
      " [0.0 0.0 1.0 -2.3186282969916334 -1.7968097268236927]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNxDRfLvKVBN9HjXcmlURF3",
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
